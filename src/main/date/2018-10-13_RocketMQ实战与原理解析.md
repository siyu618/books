RocketMQ实战与原理解析

#1. 快速入门
   * 消息队列功能介绍
      * 应用解耦
      * 流量削峰
      * 消息分发
   * rocketMQ简介
      * Notify(2007)->Napoli(2010)->MetaQ(2011)->RocketMQ(2012)->RocketMQ开源(2016)
   * quick start (见官网)
      * NameServer -> Broker

#2. 生产环境下的配置和使用
   * RocketMQ各部分角色介绍
      * NameServer
         * 多级部署，防单点
      * Broker
         * Master、Slave：BrokerName相同，brokerId不同（master的brokerId=0，其他>0）
      * Producer
      * Consumer
      * Topic
      * MessageQueue
   * 多机部署
      * 启动多个NameServer和Broker
      * 配置参数介绍
         * namesrvAddr，多个以防止单点
         * brokerClusterName，每个cluster供一个业务集群使用
         * brokerName，brokerId
         * fileReservedTime，在磁盘上保存消息的时长，in hours
         * deleteWhen，与fileReservedTime对应
         * brokerRole
            * SYNC_MASTER
            * ASYNC_MASTER
            * SLAVE
         * flushDiskType，刷盘策略
            * SYNC_FLUSH，ASYNC_FLUSH（写入page-cache后即返回）
         * listenPort=10911，监听端口号
         * storePathRootDir
   * 发送/接收消息示例
      * groupName、NameServer
   * 常用管理命令
      * mqadmin [updateTopic,deleteTopic,updateSubGroup,deleteSubGroup,updateBrokerConfig,updateTopicPerm,topicRoute,topicList,topicStats,printMsg,queryMsgById,clusterList]
   * 通过图形界面管理集群
      * rocketmq-console

#3. 用适合的方式发送和接收消息
   * 不同类型的消费者
      * DefaultMQPushConsumer、DefaultMQPullConsumer：使用者对读取操作的控制不同
      * DefaultMQPushConsumer使用
         * 由系统控制读操作，收到消息后自动调用处理函数
         * 自动提交offset
         * 自动做负载均衡，在有新的DefaultMQPushConsumer加入的时候
         * 参数
            * GroupName + MessageModle
               * clustering：group中的成员均衡消费，每个成员消费部分数据（无overlap）
               * broadcast：group中的成员都消费全量数据
            * NameServer
            * Topic
      * DefaultMQPushConsumer处理流程
         * pullMessage函数中调用PullCallBack
         * 长轮询的方式获取
            * push vs. pull
            * long polling : client主动去broker拉去数据，同时borker可以控制等待（其实是由broker和client共共同控制的？），这点跟kafka的方式还是很类似的
            * 长轮询的缺点：hold住consumer请求的时候需要占用资源
      * DefaultMQPushConsumer的流量控制
         * 每个MessageQueue有一个ProcessQueue(Map(offset->Msg), readWriteLock)，通过processQueue进行pull控制：消息数、消息大小、offset跨度
      * DefaultMQPullConsumer
         * 读取操作中的大部分功能由使用者自主控制
         * 流程
            * 获取Message Queue并遍历
            * 维护OffsetStore
            * 根据不同的消息状态做不同的处理
         * 需要注意的事项较多，各种情况需要考虑到
   * 不同类型的生产者
      * DefaultMQProducer
         * 设置producer的groupName
         * 设置instanceName，通过设置不同的Instancename来区分不同的producer，不设置默认为DEFAULT
         * 设置发送失败重试次数
         * 设置nameServer地址
         * 组装消息并发送
      * 同步发送 vs 异步发送
      * 发送消息的返回状态
         * FLUSH_DISK_TIMEOUT：刷盘失败，仅在SYNC_FLUSH才会报该错误
         * FLUSH_SLAVE_TIMEOUT：主备模式下SYNC_MASTER才会报该错误
         * SLAVE_NOT_AVAILABLE：主备模式下，SYNC_MASTER才会报该错误
         * SEND_OK：发送成功
      * 发送延迟消息
         * broker收到这类消息后，延迟一段时间再处理
            * broker 挂了就没了？？
         * setDelayLevel
            * 开源版支持有限，内部版支持更细力度
      * 自定义消息发送规则
         * MessageQueueSelector
            * 类似于MR中的Partitioner
      * 对事务的支持
         * 采用两阶段提交的方式实现事务消息：TransactionMQProducer
            * producer向rokcketMQ发送“待确认”消息，rocketMQ收到该“待确认”消息，将其持久化之后返回OK
            * producer开始执行本地事件逻辑
            * producer根据本地事件的结果向rocketMQ发送二次确认消息（commit or rollback），rocketmq收到之后或标记为可投递，或者删除该消息
            * 如果出现异常情况，比如二次确认没有到达rocketmq，服务器在经过固定的时间段之后将对“待确认”消息发起回查请求
               * 发送方在收到回查请求之后（如果阶段一的procuer挂了，回查请求会发送到同一个group中的其他producer），根据返回的结果进行commit or rollback
         * rocket 4.3 进行了优化处理，减少了磁盘脏页面 [link](https://mp.weixin.qq.com/s?__biz=MzI4MTY5NTk4Ng==&mid=2247488985&amp;idx=1&amp;sn=cafd8ce4b47bf098c7e87846318eff4d&source=41#wechat_redirect)
            * LocalTranscationExecuter
               * LocalTransactionState.{COMMIT_MESSAGE,TOLLBACK_MESSAGE}
            * TransactionMQProducer
            * TranscationCheckListener
   * 如何存储队列位置信息
      * OffsetStore
         * LocalFileOffsetStore：DefaultMQPushConsumer broadcasting模式
         * RemoteBrokerOffsetStore：DefaultMQPushConsumer clustering模式
      * setConsumeFromWhere
         * 优先级默认在offset store之后
   * 自定义日志输出
      * slf4j

#4. 分布式消息队列的协调者
   * NameServer的功能
      * NameServer可以部署多个，相互之间独立，其他同时向多个NameServer上报状态信息，从而大大热备份的目的；NameServer本身是无状态的
      * 集群状态的存储结构：RouteInfoManager
         * Map<String/*topic*/,List<QueueData>> topicQueueTable
         * Map<String/*brokerName*/,BrokerData> brokerAddrTable
         * Map<String/*clusterName*/,Set<String/*BrokerName*/> clusterAddrTable
         * Map<String/*brokerAddr*/,BrokerLiveInfo> brokerLiveTable
         * Map<String/*brokerAddr*/,List<String>/*filterServers*/> filterServerTable
      * 状态维护逻辑
         * DefaultRequestProcessor
   * 各个角色之间的交互流程
      * 比如topic的创建：broker处理，然后向nameServer注册
      * 为何不用ZK
         * rocketMQ没有使用master选举之列的功能。减少了维护的成本
   * 底层通信机制
      * remoting模块
         * RemotingClient & RemotingServer ： based on NettyRemotingAbstract
         * RemotingCommand： request and response
      * 协议涉及和实现
         * |length(4bytes)|header length(4bytes)|header data|body data|
            * length （big endian）： 第2、3、4部分长度之和
            * header length（big endian）：第3部分长度
      * netty lib

#5. 消息队列的核心机制
   * 消息存储和发送：
      * 磁盘速率
         * 顺序写600M/s，随机写100kb/s
         * 用户态 内核态，zero copy
   * 消息存储结构
      * 一个commit log：尽量顺序写，随机读
         * commitlog顺序写，可以大大提高写入效率
         * 虽然是随机读，但是利用OS的pagecache机制，可以批量从磁盘读取，作为cache存到内存中，加速后续的读取速率
         * commitlog 中存储了consumer queues、message key、tag等多有的信息
            * 所以即使consume queue丢失，也可根据commit log重建consume queue
      * consume queue中只存了offset，consumer queue可以被全部读入内存
         * record format：|commit log offset(8 bytes)|size(4bytes)|tag hashcode(4 bytes)|
   * 高可用性
      * broker 通过master 和slave达到高可用
         * brokerId(master = 0，slave > 0)
         * master 支持读写，slave支持读
      * consumer配置中，不需要配置是从master还是slave读取，当master不可用或者繁忙时，consumer会自动切换到slave
         * 消费端高可用
      * 将topic的多个message queue创建到多个broker组上
         * producer端高可用
   * 同步刷盘和异步刷盘（flushDiskType）
      * 异步刷盘
         * 写入pagecache即返回，写操作的返回快，吞吐量大，由系统控制刷盘。有丢失的可能。
      * 同步刷盘
         * 返回写成功状态时，消息已经被写入磁盘。消息人写入内存的pagecache后，立即通知刷盘，灯刷盘成功之后，刷盘线程唤醒等待的线程，返回写成功状态。
   * 同步复制vs异步复制
      * 同步复制是灯master和slave均写成功之后才反馈客户端写成功状态
         * 如果master故障，slave上有全部的备份数据，容易恢复
         * 增加了数据写入延时，降低了系统吞吐量
      * 异步复制是只要master写成功之后即反馈客户端写成功状态
         * 低延迟、高吞吐量
         * 有可能会丢失数据

#6. 可靠性优先的使用场景
   * 顺序消息
      * 全局顺序消息
         * 默认8个写队列，8个读队列
         * 要保证全局顺序消息，需要将读写队列数设置为1，同时producer和consumer的兵法也设置为1
      * 部分顺序消息
         * 需要producer和consumer配合
            * producer：MEssageQueueSelector
            * consumer：MessageListenerOrderly
               * 每个MessageQueue有一个锁，保证同一个consumerQueue不菲被并发处理
   * 消息重复消费问题
      * 确保一定投递，保证消息不丢失，但有可能消息重复
      * producer setRetryTimesWhenSendFailed
      * 解决重复消费的方法（都需要consumer自己实现）
         * 方法1. 保证消费逻辑的幂等性（多次调用和一次调用效果相同）
         * 方法2. 维护一个已消费消息的记录，消费前查询这个消息是否被消费过
   * 动态增加机器
      * 动态增加NameServer
         * 四种方式设置NameSever（优先级由高到低）
            * 代码设置 producer.setNamesrvAddr
            * 使用java启动参数， -Drocketmq.namesrv.addr
            * linux环境变量，NAMESRV_ADDR
            * 通过http服务来设置：rocketmq.namesrv.domain, roketmq.namesrv.domain.subgroup
      * 动态增减broker
         * 减少broker是否会丢失数据取决于发送方式：sendOneWay， 异步
   * 各种故障对消息的影响
      * 多master， 每个master带有slave
      * 主从之间设置成SYNC_MASTER
      * procuer用同步方式发送
      * 刷盘策略设置成SYNC_FLUSH
   * 消息优先级
      * 分拆到不同的topic
      * 分拆到不同的message queue
      * 强优先级：程序自主控制

#7. 吞吐量优先的使用场景
   * 在broker端进行消息过滤
      * 消息的tag和key
         * tag用于consumer的过滤
         * key用于通过命令行查询消息
      * 通过tag进行过滤
         * consumerQueue的存储格式
            * | commitLog offset（8 bytes)|size(4 bytes)|message Tage hash code(8 bytes)|
            * broker端通过hashcode过滤，consumer通过对比message tag字符串消除hash冲突
      * 用sql表达式的方式进行过滤
         * msg.putUserProperty("a", "Xxx");
         * 支持类sql的过滤，在broker端进行，增大了磁盘压力
      * filter server方式过滤
         * broker端同时启动多个filter Server
   * 提高consumer处理能力
      * 提高并行度
         * consumer数量不超过read queue数量，通过提高并发度（consumeThreadmin, consumerThreadMax)
      * 以批量方式进行消费
         * consumerMessageBatchMaxSize
      * 检测延时情况，跳过非重要消息
   * consumer的负载均衡
      * rocketmq中，负载均衡或者消息分配是在consumer端代码中完成的
         * consumer从broker处获取全局信息，然后自己做负载均衡，只处理分给自己的那部分消息
      * DefaultMQPushConsumer的负载均衡
         * 每一个DefaultMQPushConsumer启动后，会马上触发一个doReblance动作
            * AllocateMessageQueueAveragely
            * AllocateMessageQueueAveragelyByCircle
            * AllocateMessageQueueByConfig
            * AllocateMessageQueueByMachineRoom
            * AllocateMessageQueueConsistenHash
      * DefaultMQPullConsumer的负载均衡
         * pull consumer可以看到所有的mess个queue，而且consumer控制从哪个messagequeue读取消息，同时也可以控制offset和负载均衡策略
         * resisterMessageQueueListener
         * MQPullConsumerScheduleService
   * 提高producer的发送速率
      * 发送的三步骤
         * 客户端发送请求
         * 服务端处理请求
         * 服务器向客户端返回应答
      * sendOneWay：写入客户端的socket就返回
      * 增加producer的并发量，异步刷盘
      * linux OS 使用EXT4文件系统，io调度算法使用deadline算法
         * deadline
            * 实现四个队列，两个处理read&write，另外两个处理超时的read&write
            * 正常的read、write中，元素按扇区好排序，进行正常的IO合并处理以提高吞吐量
            * 因为IO请求会集中在某些磁盘位置，这样会导致新来的请求一直被合并，可能会有其他磁盘位置的io请求被饿死。
            * 超时的read 和write队列，元素按请求创建的时间排序，如果超时出现，就放入这两个队列，调度算法保证超时（达到最终期限时间）的队列中的IO请求会有优先被处理
   * 系统性能调优的一般流程
      * 搭建测试环境，查看硬件利用率
      * 模拟实际情况，并且逐步增大请求量，同时检测系统的TPS
      * 在TPS峰值的时候，保持系统在峰值状态下运行，并查看以下指标
         * top 查看CUP和内存的使用率（小米的 falcon）
         * sar 查看网卡使用情况
            * netstat
            * iperf3
         * iostat 查看磁盘使用（falcon）
         * 程序本身的监控
            * jvisualvm， jstack，perfJ

#8. 和其他系统交互
   * springboot 中使用RocketMQ
      * rocketmq-client
      * spring messaging：spring-boot-starter-rocketmq
   * 云上使用RokectMQ
   * RocketMQ与spark、flink对接
      * connector 开发中。。。 
   * 自定义开发运维工具
      * rocketmq-console
      * tools

#9. 首个Apache中间件顶级项目

#10. NameServer源码解析
   * 模块入口代码的功能
   * NameServer的总控逻辑
      * 8个线程的线程池
      * 一个扫描实效的broker（scanNotActiveBroker）
      * 一个打印配置信息（printAllPeriodically）
   * 核心业务逻辑处理
      * DefaultRequestProcessor
   * 集群存储状态
      * RoutInfoManager

#11. 最常用的消费类
   * RocketMQPushConsumer
   * 整体逻辑
      * DefaultMQPushPushConsumerImpl
         * 初始化MQClientInstance
         * offsetStore（broadcasting：LocalFileOffsetStore，clustering：RemoteBrokerOffsetStore）
         * 初始化consumerMessageService
         * 获取消息：pullMessage（流量控制 + 处理）
   * 消息的并发处理
      * MessageConcurrentlyService
         * 三个线程池
            * 正常处理收到的消息
            * 处理推迟消费的信息
            * 定期清理超时消息(15min)
      * 消息处理的返回值
            * CONSUME_SUCCESS
            * RECONSUMER_LATER
      * ProcessQueue
         * 每个MessageQueue都有一个ProcessQueue与之对应
            * TreeMap<Long, MessageExt>
            * ReadWriteLock lockTreeMap
   * 生产者消费者的底层类
      * producer + consumer都需要和broker协作
      * MQClientInstance
         * producer和consumer的底层类
         * MQClientManager.getInstance.getAndCreateMQClientInstance
            * ConcurrentMap<String/*clientId*/, MQClientInstance>
         * 功能
            * MQClientAPIImpl，负责底层的通信
            * pullMessageService
            * RebalanceService
            * 定时任务：MQClientInstance.this.mqClientAPIImpl.
               * fetchNameServerAddr()
               * updateTopicRouteInfoFromServer
               * cleanOfflineBroker & sendHeartbeatToAllBrokerWithLock
               * persistAllConsumerOffset
               * adjustThreadPool

#12. 主从同步机制
   * Broker的Master、Slave
   * 同步属性信息
      * message、TopicConfig、ConsumerOffset、DelayOffset、SubsciptionGroupConfig
      * slaveSynchronize.syncAll()
         * syncTopicConfig
         * syncConsumerOffset
         * syncDelayOffset
         * syncSubscriptionGroupConfig
      * 基于netty command实现
   * 同步消息体
      * commitLog：数据量大、对于实时性和可靠性要求高
      * HAService：实现commitLog同步的主体
         * 直接使用TCP连接，效率更高
   * sync_master vs. async_master
      * commitLog类的putMessage函数末尾调用handleHA（）

#13. 基于Netty的通信实现
   * 详见netty in action

